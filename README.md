# Ellipsoidal Intersection Benchmarking

This repository contains an experimental framework for benchmarking five algorithms that compute the optimal intersection radius of non-homogeneous ellipsoids in arbitrary dimension. The C++ implementation generates random ellipsoid instances, evaluates each solver across a grid of values for the number of ellipsoids and the ambient dimension, measures runtimes, and writes empirical statistics to a CSV file. A separate Python analysis script consumes this CSV and produces publication-ready figures and LaTeX tables.

The experiment is designed for reproducibility. All build steps are handled by CMake, and all analysis steps run inside a local Python virtual environment. Two driver scripts orchestrate the workflow.

## Dependencies and Versions

The C++ code has been tested on macOS with the following Homebrew packages:

- NLopt 2.10.0
- Boost 1.89.0_1
- Eigen 5.0.1

The Python analysis uses the following versions:

- Python 3.13
- NumPy 2.3.4
- pandas 2.3.3
- Matplotlib 3.10.7

A virtual environment ensures these versions remain stable for all users.

## Building with CMake

From the repository root, configure and build using the standard out-of-source workflow:

    mkdir -p build
    cd build
    cmake -DCMAKE_BUILD_TYPE=Release ..
    cmake --build . --config Release -j8

This produces the executable:

    build/output/benchmark_stats2

VS Code users may rely on the CMake Tools extension, which automatically configures and builds the project.

## Running the Benchmark

The executable accepts a single integer argument specifying the number of random trials per grid point. Running it produces a file named:

    benchmark_results.csv

Two helper scripts are provided:

- `run_cpp_only.sh` runs only the C++ benchmark after the project has been configured and built.
- `run_experiment.sh` builds the project if necessary, runs the benchmark, activates or creates a virtual environment, installs required Python packages, and runs the renderer that generates figures and LaTeX tables.

For example:

    ./run_experiment.sh 50

If no argument is provided, the default is 50 trials.

## Python Analysis

The analysis script:

    make_plots.py

reads `benchmark_results.csv`, computes aggregated statistics, and writes:

- figures into `figs/`
- LaTeX tables into `tables/`

This step always runs inside the project’s virtual environment.

## Directory Structure

    ellph/
        CMakeLists.txt
        src/
        include/
        benchmark_stats2.cpp
        make_plots.py
        run_cpp_only.sh
        run_experiment.sh
        figs/
        tables/
        build/               (generated by CMake)

## Reproducibility

This repository fixes compiler, library, and Python package versions through CMake, Homebrew, and a project-local virtual environment. The driver scripts define a deterministic “build → benchmark → analyse” pipeline. Another researcher can clone the project, install the same Homebrew packages, execute the same scripts, and reproduce the entire experiment.

A metadata file recording timestamps, compiler versions, and Python package versions can be added if desired.